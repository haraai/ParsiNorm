<div dir='ltr' align='left'>

# PerSpeechNormalization
The normalization step is so essential to format unification in pure textual applications. However, for embedded language models in speech processing modules, normalization is not limited to format unification. Moreover, it has to convert each readable symbol, number, etc., to how they are pronounced. 

<h1>functionalities</h1>

<h3> General Normalization</h3>

+ Sentence tokenizer
+ Normalizing persian and English characters 
+ Normalizing Numbers (Converting to unique perisan Number)
+ Converting Persian, English, Arabic symbols to normalized characters
+ Normalize Punctuations
+ Removing emojis
+ Converting HTML tags to characters and symbols
+ Having unique floating point number
+ Removing different comma between numbers
+ Removing repeated punctuations

<h3> Speech Normalization</h3>

+ Converting mail and url to how are pronounced
+ Converting Date and Times to how they are pronounced
+ Converting special numbers to how they are pronounced
+ Converting English and Persian Abbrevations to how they are pronounced
+ converting telephone numbers to how they are pronounced in Persian
+ Converting currency to how they are read
+ Converting some symbols to how they are read such as %, Â°, *, #, +, &, Î”

<h1>Usage</h1>

```python
>>> from mail_url_cleaner import mail_url_cleaner
>>> mail_url_cleaner = mail_url_cleaner()
>>> mail_url_cleaner.find_mails_clean(sentence="info@hara.ai")
info at hara dot ai

>>> mail_url_cleaner.find_urls_clean(sentence="https://hara.ai/#services")
https do noghte slash slash hara dot ai

>>> from date_time_to_text import date_time_to_text
>>> date_time_to_text = date_time_to_text()
>>> date_time_to_text.date_to_text(sentence='2021/10/27')
Ø¨ÛŒØ³Øª Ùˆ Ù‡ÙØªÙ… Ø§Ú©ØªØ¨Ø± Ø³Ø§Ù„ Ø¯Ùˆ Ù‡Ø²Ø§Ø± Ùˆ Ø¨ÛŒØ³Øª Ùˆ ÛŒÚ©

>>> date_time_to_text.time_to_text(sentence='22:57:11')
Ø¨ÛŒØ³Øª Ùˆ Ø¯Ùˆ Ùˆ Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ù‡ÙØª Ø¯Ù‚ÛŒÙ‚Ù‡ Ùˆ  ÛŒØ§Ø²Ø¯Ù‡ Ø«Ø§Ù†ÛŒÙ‡

>>> from general_normalization import general_normalization
>>> general_normalization = general_normalization()
>>> general_normalization.alphabet_correction(sentence='ï»™ï¯˜Ý™Ý¤ï®®')
Ú©ÙˆØ¯Ú©ÛŒ

>>> general_normalization.english_correction(sentence='naÃ¯ve')
naive

>>> general_normalization.html_correction(sentence='&quot;')
"

>>> general_normalization.arabic_correction(sentence='ï·º')
ØµÙ„ÛŒ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÛŒÙ‡ Ùˆ Ø³Ù„Ù…

>>> general_normalization.punctuation_correction(sentence="â€¦")
...

>>> general_normalization.specials_chars(sentence="â„¡")
TEL

>>> general_normalization.remove_emojis(sentence='ðŸ˜Š')


>>> general_normalization.unique_floating_point(sentence='1ØŒ2')
Û±.Û²

>>> general_normalization.remove_comma_between_numbers(sentence='1Ù¬234')
Û±Û²Û³Û´

>>> general_normalization.number_correction(sentence="â‘¤")
Ûµ

>>> general_normalization.remove_not_desired_chars(sentence="^ Hi ~")
  Hi  

>>> general_normalization.remove_repeated_punctuation(sentence="!!!!!")
!

>>> from telephone_number import telephone_number
>>> telephone_number = telephone_number()
>>> telephone_number.find_phones_replace(sentence='ØªÙ„ÙÙ† Û°Û²Û±Û³Û³Û´ÛµÛ¶Û·Û¸Û¸')
ØªÙ„ÙÙ†   ØµÙØ±  Ø¨ÛŒØ³Øª Ùˆ ÛŒÚ© Ø³ÛŒ Ùˆ Ø³Ù‡ Ú†Ù‡Ù„ Ùˆ Ù¾Ù†Ø¬ Ø´ØµØª Ùˆ Ù‡ÙØª Ù‡Ø´ØªØ§Ø¯ Ùˆ Ù‡Ø´Øª

>>> from abbreviation import abbreviation
>>> abbreviation = abbreviation()
>>> abbreviation.replace_date_abbreviation(sentence=".Ø¯Ø± Ø³Ø§Ù„ 1400 Ù‡.Ø´")
Ø¯Ø± Ø³Ø§Ù„ 1400 Ù‡Ø¬Ø±ÛŒ Ø´Ù…Ø³ÛŒ

>>> abbreviation.replace_persian_label_abbreviation(sentence='Ø§Ù…Ø§Ù… Ø²Ù…Ø§Ù† (Ø¹Ø¬)')
Ø§Ù…Ø§Ù… Ø²Ù…Ø§Ù†  Ø¹Ø¬Ù„ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„ÛŒ ÙØ±Ø¬Ù‡ Ø§Ù„Ø´Ø±ÛŒÙ 

>>> abbreviation.replace_law_abbreviation(sentence='Ø¯Ø± Ù‚.Ø§ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª')
Ø¯Ø± Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³Ø§Ø³ÛŒ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª

>>> abbreviation.replace_book_abbreviation(sentence='Ø¨Ù‡ Ú©ØªØ§Ø¨ Ø²ÛŒØ± Ø±.Ú© Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯')
Ø¨Ù‡ Ú©ØªØ§Ø¨ Ø²ÛŒØ± Ø±Ø¬ÙˆØ¹ Ú©Ù†ÛŒØ¯ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯

>>> abbreviation.replace_other_abbreviation(sentence='Ø¯Ø± Ù‚Ø§Ù†ÙˆÙ† Ø¬.Ø§ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª')
Ø¯Ø± Ù‚Ø§Ù†ÙˆÙ† Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª

>>> abbreviation.replace_English_abbrevations(sentence='U.S.A')
ÛŒÙˆ Ø§Ø³ Ø¢

>>> from TTS_normalization import TTS_normalization
>>> TTS_normalization = TTS_normalization()
>>> TTS_normalization.math_correction(sentence='â…ž')
Ù‡ÙØª Ù‡Ø´ØªÙ…

>>> TTS_normalization.replace_currency(sentence='Û³Û³$')
Û³Û³ Ø¯Ù„Ø§Ø±

>>> TTS_normalization.replace_symbols(sentence='Û³Û³Â°')
Û³Û³ Ø¯Ø±Ø¬Ù‡ 

>>> from special_numbers import special_numbers
>>> special_numbers = special_numbers()
>>> special_numbers.convert_numbers_to_text(sentence='122')
 ØµØ¯ Ùˆ Ø¨ÛŒØ³Øª Ùˆ Ø¯Ùˆ

>>> special_numbers.replace_national_code(sentence='0499370899')
ØµÙØ±  Ú†Ù‡Ø§Ø±   Ù†Ù‡ØµØ¯ Ùˆ Ù†ÙˆØ¯ Ùˆ Ø³Ù‡   Ù‡ÙØªØ§Ø¯   Ù‡Ø´ØªØµØ¯ Ùˆ Ù†ÙˆØ¯ Ùˆ Ù†Ù‡

>>> special_numbers.replace_card_number(sentence='6037701689095443')
Ø´ØµØª   Ø³ÛŒ Ùˆ Ù‡ÙØª   Ù‡ÙØªØ§Ø¯   Ø´Ø§Ù†Ø²Ø¯Ù‡   Ù‡Ø´ØªØ§Ø¯ Ùˆ Ù†Ù‡   ØµÙØ±  Ù†Ù‡   Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ú†Ù‡Ø§Ø±   Ú†Ù‡Ù„ Ùˆ Ø³Ù‡

>>>special_numbers.replace_shaba(sentence='IR820540102680020817909002')
 Ø¢ÛŒ Ø¢Ø±   Ù‡Ø´ØªØ§Ø¯ Ùˆ Ø¯Ùˆ   ØµÙØ±  Ù¾Ù†Ø¬   Ú†Ù‡Ù„   Ø¯Ù‡   Ø¨ÛŒØ³Øª Ùˆ Ø´Ø´   Ù‡Ø´ØªØ§Ø¯   ØµÙØ±  Ø¯Ùˆ   ØµÙØ±  Ù‡Ø´Øª   Ù‡ÙØ¯Ù‡   Ù†ÙˆØ¯   Ù†ÙˆØ¯   ØµÙØ±  Ø¯Ùˆ 

>>> from Tokenizer import Tokenizer
>>> tokenizer = Tokenizer()
>>> tokenizer.sentence_tokenize('.Ø§ÛŒÙ† Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡ Ø§Ø³Øª. Ø¨Ø§ÛŒØ¯ Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ú¯ÛŒØ±Ø¯. Ø¯Ø±Ø³Øª Ø§Ø³Øª') 
['Ø§ÛŒÙ† Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡ Ø§Ø³Øª .', ' Ø¨Ø§ÛŒØ¯ Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ú¯ÛŒØ±Ø¯ .', ' Ø¯Ø±Ø³Øª Ø§Ø³Øª .']

```


<h1> Reference </h1>

If you use or discuss this normalization tool in your work, please cite our paper :

```
@article{oji2021perspeechnorm,
  title={PerSpeechNorm: A Persian Toolkit for Speech Processing Normalization},
  author={Oji, Romina and Razavi, Seyedeh Fatemeh and Dehsorkh, Sajjad Abdi and Hariri, Alireza and Asheri, Hadi and Hosseini, Reshad},
  journal={arXiv preprint arXiv:2111.03470},
  year={2021}
}
```

<h1> Contact </h1>

If you have any technical question regarding the dataset or publication, please
create an issue in this repository.


</div>
